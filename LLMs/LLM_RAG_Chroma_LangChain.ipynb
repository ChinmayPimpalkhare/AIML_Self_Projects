{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ztMlVtfs1PHP"
      },
      "outputs": [],
      "source": [
        "# === SETUP ===\n",
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\"\n",
        "\n",
        "!pip install langchain chromadb sentence-transformers faiss-cpu huggingface_hub pypdf\n",
        "!pip install -U langchain-community\n",
        "\n",
        "# First install Gradio\n",
        "!pip install gradio\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.chains import RetrievalQA\n",
        "import gradio as gr\n",
        "from langchain.document_loaders import PyPDFLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tD18aXcH5OSL"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "from transformers import pipeline\n",
        "\n",
        "# === SIMPLE DATASET ===\n",
        "sample_text = \"\"\"\n",
        "Stanford University is a private research university in Stanford, California.\n",
        "It is known for its academic strength, proximity to Silicon Valley, and ranking among the world's top universities.\n",
        "\"\"\"\n",
        "with open(\"stanford_info.txt\", \"w\") as f:\n",
        "    f.write(sample_text)\n",
        "\n",
        "loader = TextLoader(\"stanford_info.txt\")\n",
        "documents = loader.load()\n",
        "\n",
        "# === CHUNKING ===\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# === EMBEDDING MODEL ===\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# === VECTOR DB (Chroma) ===\n",
        "db = Chroma.from_documents(texts, embedding_model)\n",
        "\n",
        "# === LOCAL SMALL LLM (Flan-T5 small) ===\n",
        "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
        "\n",
        "# === SIMPLE retrieval + QA manually ===\n",
        "def retrieve_and_answer(query):\n",
        "    docs = db.similarity_search(query, k=3)\n",
        "    context = \"\\n\\n\".join([d.page_content for d in docs])\n",
        "    prompt = f\"Answer based on context:\\n{context}\\n\\nQuestion: {query}\"\n",
        "    result = generator(prompt, max_length=200)[0]['generated_text']\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRgihctu34Yv",
        "outputId": "e659ba53-632d-4e3c-f51b-9d46f0e43f6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ask me anything (type 'exit' to quit): where is stanford\n",
            "\n",
            "ðŸ”Ž Answer: Stanford, California\n",
            "\n",
            "Ask me anything (type 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "# === INTERACTIVE TEST ===\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nAsk me anything (type 'exit' to quit): \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    answer = retrieve_and_answer(query)\n",
        "    print(f\"\\nðŸ”Ž Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "Pco9cZzI2LXm",
        "outputId": "c4924c49-2102-40aa-a67a-80ca6d4df114"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f96b2fa2fcc5b72257.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://f96b2fa2fcc5b72257.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# (Assuming previous code is already run and you have `retrieve_and_answer` defined)\n",
        "\n",
        "# Define a Gradio function\n",
        "def qa_gradio(query):\n",
        "    answer = retrieve_and_answer(query)\n",
        "    return answer\n",
        "\n",
        "# Launch Gradio app\n",
        "iface = gr.Interface(\n",
        "    fn=qa_gradio,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask a question about Stanford...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Mini RAG Chatbot\",\n",
        "    description=\"Ask questions about the document loaded!\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PivhO6Bv4Vox",
        "outputId": "3dbdd77a-39ad-49b2-925a-8486aac13693"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Question: Where is Stanford located?\n",
            "Answer: Stanford, California\n",
            "\n",
            "Question: What is Stanford famous for?\n",
            "Answer: academic strength, proximity to Silicon Valley, and ranking among the world's top universities\n",
            "\n",
            "âœ… Simple Retrieval QA Accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "# Simple evaluation set\n",
        "eval_set = [\n",
        "    {\"question\": \"Where is Stanford located?\", \"expected_keywords\": [\"California\"]},\n",
        "    {\"question\": \"What is Stanford famous for?\", \"expected_keywords\": [\"Silicon Valley\", \"academic strength\"]},\n",
        "]\n",
        "\n",
        "# Evaluate\n",
        "correct = 0\n",
        "\n",
        "for item in eval_set:\n",
        "    answer = retrieve_and_answer(item['question'])\n",
        "    print(f\"\\nQuestion: {item['question']}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "\n",
        "    if any(keyword.lower() in answer.lower() for keyword in item['expected_keywords']):\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / len(eval_set)\n",
        "print(f\"\\nâœ… Simple Retrieval QA Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
